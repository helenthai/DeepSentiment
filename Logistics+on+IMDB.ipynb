{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"data/imdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a waste of talent -- although it appears ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No matter how you feel about Michael Jackson h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contrary to what many may believe as this movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10 for this film.&lt;br /&gt;&lt;br /&gt;i'm a british ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In theory, 'Director's Commentary' should have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  What a waste of talent -- although it appears ...          0\n",
       "1  No matter how you feel about Michael Jackson h...          1\n",
       "2  Contrary to what many may believe as this movi...          1\n",
       "3  10/10 for this film.<br /><br />i'm a british ...          1\n",
       "4  In theory, 'Director's Commentary' should have...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + MNB (using movie review data as trainning data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data to training set and testing set\n",
    "def review_series_to_list(review_series):\n",
    "    review_list=[]\n",
    "    n_review = len(review_series)\n",
    "    for i in range(0,n_review):\n",
    "        review_list.append(review_series[i])\n",
    "    return review_list  \n",
    "\n",
    "train_review_list = review_series_to_list(raw['text'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_review_list, raw['sentiment'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_model = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "nb_fit = nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_model_bow = Pipeline([('vect', CountVectorizer()),\n",
    "                    \n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "nb_fit_bow = nb_model_bow.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_predicted2 = nb_model_bow.predict(X_test)\n",
    "nb_accuracy2 = np.mean(nb_predicted2 == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859515151515\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Rating < 5(0)       0.83      0.89      0.86      8171\n",
      "Rating >=7(1)       0.89      0.83      0.86      8329\n",
      "\n",
      "  avg / total       0.86      0.86      0.86     16500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7299,  872],\n",
       "       [1446, 6883]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Prediction and evaluation\n",
    "nb_predicted = nb_model.predict(X_test)\n",
    "nb_accuracy = np.mean(nb_predicted == y_test) \n",
    "print (nb_accuracy)\n",
    "print(metrics.classification_report(y_test, nb_predicted,\n",
    "    target_names=['Rating < 5(0)','Rating >=7(1)']))\n",
    "metrics.confusion_matrix(y_test, nb_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + logistics (using movie review data as trainning data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "lr_model = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LR()),\n",
    "])\n",
    "\n",
    "lr_fit = lr_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892484848485\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Rating < 5(0)       0.90      0.89      0.89      8171\n",
      "Rating >=7(1)       0.89      0.90      0.89      8329\n",
      "\n",
      "  avg / total       0.89      0.89      0.89     16500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7234,  937],\n",
       "       [ 837, 7492]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_predicted = lr_model.predict(X_test)\n",
    "lr_accuracy = np.mean(lr_predicted == y_test) \n",
    "print (lr_accuracy)\n",
    "print(metrics.classification_report(y_test, lr_predicted,\n",
    "    target_names=['Rating < 5(0)','Rating >=7(1)']))\n",
    "metrics.confusion_matrix(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrl2_model = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LR(penalty = 'l2', dual = True, random_state = 0)),\n",
    "])\n",
    "\n",
    "lrl2_fit = lrl2_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.892484848485\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Rating < 5(0)       0.90      0.89      0.89      8171\n",
      "Rating >=7(1)       0.89      0.90      0.89      8329\n",
      "\n",
      "  avg / total       0.89      0.89      0.89     16500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7234,  937],\n",
       "       [ 837, 7492]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrl2_predicted = lrl2_model.predict(X_test)\n",
    "lrl2_accuracy = np.mean(lrl2_predicted == y_test) \n",
    "print (lrl2_accuracy)\n",
    "print(metrics.classification_report(y_test, lr_predicted,\n",
    "    target_names=['Rating < 5(0)','Rating >=7(1)']))\n",
    "metrics.confusion_matrix(y_test, lr_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89031488,  0.89238806,  0.88865672,  0.8880597 ,  0.89386476])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5 = model_selection.cross_val_score(lrl2_fit, X_train, y_train, cv=5)\n",
    "scores5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_values = {'C':[30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helen/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/helen/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "model_LR = GridSearchCV(LR(penalty = 'l2', dual = True, random_state = 0), \n",
    "                        grid_values, scoring = 'roc_auc', cv = 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## BOF + MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) #revew_text \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helen/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/helen/anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "clean_review = review_to_words(raw[\"text\"][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helen/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/helen/anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = raw[\"text\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( raw[\"text\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helen/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/helen/anaconda/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 50000\n",
      "\n",
      "Review 2000 of 50000\n",
      "\n",
      "Review 3000 of 50000\n",
      "\n",
      "Review 4000 of 50000\n",
      "\n",
      "Review 5000 of 50000\n",
      "\n",
      "Review 6000 of 50000\n",
      "\n",
      "Review 7000 of 50000\n",
      "\n",
      "Review 8000 of 50000\n",
      "\n",
      "Review 9000 of 50000\n",
      "\n",
      "Review 10000 of 50000\n",
      "\n",
      "Review 11000 of 50000\n",
      "\n",
      "Review 12000 of 50000\n",
      "\n",
      "Review 13000 of 50000\n",
      "\n",
      "Review 14000 of 50000\n",
      "\n",
      "Review 15000 of 50000\n",
      "\n",
      "Review 16000 of 50000\n",
      "\n",
      "Review 17000 of 50000\n",
      "\n",
      "Review 18000 of 50000\n",
      "\n",
      "Review 19000 of 50000\n",
      "\n",
      "Review 20000 of 50000\n",
      "\n",
      "Review 21000 of 50000\n",
      "\n",
      "Review 22000 of 50000\n",
      "\n",
      "Review 23000 of 50000\n",
      "\n",
      "Review 24000 of 50000\n",
      "\n",
      "Review 25000 of 50000\n",
      "\n",
      "Review 26000 of 50000\n",
      "\n",
      "Review 27000 of 50000\n",
      "\n",
      "Review 28000 of 50000\n",
      "\n",
      "Review 29000 of 50000\n",
      "\n",
      "Review 30000 of 50000\n",
      "\n",
      "Review 31000 of 50000\n",
      "\n",
      "Review 32000 of 50000\n",
      "\n",
      "Review 33000 of 50000\n",
      "\n",
      "Review 34000 of 50000\n",
      "\n",
      "Review 35000 of 50000\n",
      "\n",
      "Review 36000 of 50000\n",
      "\n",
      "Review 37000 of 50000\n",
      "\n",
      "Review 38000 of 50000\n",
      "\n",
      "Review 39000 of 50000\n",
      "\n",
      "Review 40000 of 50000\n",
      "\n",
      "Review 41000 of 50000\n",
      "\n",
      "Review 42000 of 50000\n",
      "\n",
      "Review 43000 of 50000\n",
      "\n",
      "Review 44000 of 50000\n",
      "\n",
      "Review 45000 of 50000\n",
      "\n",
      "Review 46000 of 50000\n",
      "\n",
      "Review 47000 of 50000\n",
      "\n",
      "Review 48000 of 50000\n",
      "\n",
      "Review 49000 of 50000\n",
      "\n",
      "Review 50000 of 50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for i in range( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (  \"Review %d of %d\\n\" % ( i+1, num_reviews ) )                                                                   \n",
    "    clean_train_reviews.append( review_to_words( raw[\"text\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8449,  0.8504,  0.8439,  0.8566,  0.8498])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "NB_bow = MultinomialNB()\n",
    "NB_bow_fit = NB_bow.fit( train_data_features, raw[\"sentiment\"] )\n",
    "scores5 = cross_val_score(NB_bow_fit, train_data_features, raw[\"sentiment\"], cv=5)\n",
    "scores5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8438,  0.847 ,  0.854 ,  0.8464,  0.8486,  0.8444,  0.8528,\n",
       "        0.8614,  0.8518,  0.8466])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores10 = cross_val_score(NB_bow_fit, train_data_features, raw[\"sentiment\"], cv=10)\n",
    "scores10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 5cv : 0.85 (+/- 0.01)\n",
      "Accuracy 10cv : 0.85 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy 5cv : %0.2f (+/- %0.2f)\" % (scores5.mean(), scores5.std() * 2))\n",
    "print(\"Accuracy 10cv : %0.2f (+/- %0.2f)\" % (scores10.mean(), scores10.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOF + Logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8723,  0.8733,  0.8755,  0.8735,  0.8674])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_bow_fit = LR()\n",
    "LR_bow_fit = LR_bow_fit.fit( train_data_features, raw[\"sentiment\"] )\n",
    "scores5 = cross_val_score(LR_bow_fit, train_data_features, raw[\"sentiment\"], cv=5)\n",
    "scores5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87  ,  0.8764,  0.8766,  0.8736,  0.8752,  0.8842,  0.8758,\n",
       "        0.8816,  0.8782,  0.866 ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores10 = cross_val_score(LR_bow_fit, train_data_features, raw[\"sentiment\"], cv=10)\n",
    "scores10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 5cv : 0.87 (+/- 0.01)\n",
      "Accuracy 10cv : 0.88 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy 5cv : %0.2f (+/- %0.2f)\" % (scores5.mean(), scores5.std() * 2))\n",
    "print(\"Accuracy 10cv : %0.2f (+/- %0.2f)\" % (scores10.mean(), scores10.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
